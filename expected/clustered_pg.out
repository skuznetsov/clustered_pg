CREATE EXTENSION clustered_pg;
SELECT public.version();
      version       
--------------------
 clustered_pg 0.1.0
(1 row)

SELECT public.clustered_pg_observability() AS observability_bootstrap;
                                              observability_bootstrap                                               
--------------------------------------------------------------------------------------------------------------------
 clustered_pg=0.1.0 api=1 counters={observability=1,costestimate=0,index_inserts=0,insert_errors=0,vacuumcleanup=0}
(1 row)

SELECT (public.clustered_pg_observability() ~ 'clustered_pg=0.1.0') AS observability_probe;
 observability_probe 
---------------------
 t
(1 row)

-- ====================================================================
-- Functional regression tests: multi-type index, JOIN UNNEST rescan,
-- delete+vacuum consistency, locator edge cases, directed placement
-- ====================================================================
-- Test int2 and int4 index support (only int8 tested above)
CREATE TABLE clustered_pg_int2_smoke(id smallint) USING clustered_heap;
CREATE INDEX clustered_pg_int2_smoke_idx
	ON clustered_pg_int2_smoke USING clustered_pk_index (id);
INSERT INTO clustered_pg_int2_smoke(id) SELECT generate_series(1,20)::smallint;
SELECT count(*) AS int2_row_count FROM clustered_pg_int2_smoke;
 int2_row_count 
----------------
             20
(1 row)

SET enable_seqscan = off;
SET enable_bitmapscan = off;
SELECT count(*) AS int2_filter_eq FROM clustered_pg_int2_smoke WHERE id = 10::smallint;
 int2_filter_eq 
----------------
              1
(1 row)

SELECT count(*) AS int2_filter_range FROM clustered_pg_int2_smoke WHERE id BETWEEN 5::smallint AND 15::smallint;
 int2_filter_range 
-------------------
                11
(1 row)

RESET enable_seqscan;
RESET enable_bitmapscan;
DROP TABLE clustered_pg_int2_smoke;
CREATE TABLE clustered_pg_int4_smoke(id integer) USING clustered_heap;
CREATE INDEX clustered_pg_int4_smoke_idx
	ON clustered_pg_int4_smoke USING clustered_pk_index (id);
INSERT INTO clustered_pg_int4_smoke(id) SELECT generate_series(1,20);
SELECT count(*) AS int4_row_count FROM clustered_pg_int4_smoke;
 int4_row_count 
----------------
             20
(1 row)

SET enable_seqscan = off;
SET enable_bitmapscan = off;
SELECT count(*) AS int4_filter_eq FROM clustered_pg_int4_smoke WHERE id = 10;
 int4_filter_eq 
----------------
              1
(1 row)

SELECT count(*) AS int4_filter_range FROM clustered_pg_int4_smoke WHERE id BETWEEN 5 AND 15;
 int4_filter_range 
-------------------
                11
(1 row)

RESET enable_seqscan;
RESET enable_bitmapscan;
DROP TABLE clustered_pg_int4_smoke;
-- Test locator edge cases: zero, large values, boundary
SELECT locator_major(locator_pack(0, 0)) AS loc_zero_major,
       locator_minor(locator_pack(0, 0)) AS loc_zero_minor;
 loc_zero_major | loc_zero_minor 
----------------+----------------
              0 |              0
(1 row)

SELECT locator_cmp(locator_pack(0, 0), locator_pack(0, 0)) AS loc_cmp_equal_zero;
 loc_cmp_equal_zero 
--------------------
                  0
(1 row)

SELECT locator_cmp(locator_pack(0, 0), locator_pack(0, 1)) AS loc_cmp_zero_vs_one;
 loc_cmp_zero_vs_one 
---------------------
                  -1
(1 row)

SELECT locator_to_hex(locator_pack(9223372036854775807, 9223372036854775807)) AS loc_max_hex;
            loc_max_hex            
-----------------------------------
 7FFFFFFFFFFFFFFF:7FFFFFFFFFFFFFFF
(1 row)

SELECT locator_major(locator_pack(9223372036854775807, 0)) AS loc_max_major;
    loc_max_major    
---------------------
 9223372036854775807
(1 row)

SELECT locator_minor(locator_pack(0, 9223372036854775807)) AS loc_max_minor;
    loc_max_minor    
---------------------
 9223372036854775807
(1 row)

SELECT locator_to_hex(locator_next_minor(locator_pack(0, 0), 1)) AS loc_next_from_zero;
        loc_next_from_zero         
-----------------------------------
 0000000000000000:0000000000000001
(1 row)

SELECT locator_to_hex(locator_advance_major(locator_pack(0, 5), 1)) AS loc_advance_from_zero;
       loc_advance_from_zero       
-----------------------------------
 0000000000000001:0000000000000005
(1 row)

-- Test JOIN UNNEST correctness
CREATE TABLE clustered_pg_join_unnest_base(id bigint) USING clustered_heap;
CREATE INDEX clustered_pg_join_unnest_base_idx
	ON clustered_pg_join_unnest_base USING clustered_pk_index (id);
INSERT INTO clustered_pg_join_unnest_base(id) SELECT generate_series(1,100);
-- Probe with array of keys via JOIN (exercises rescan on inner side)
SET enable_seqscan = off;
SET enable_bitmapscan = off;
SET enable_hashjoin = off;
SET enable_mergejoin = off;
SELECT count(*) AS join_unnest_hit_count
FROM clustered_pg_join_unnest_base b
JOIN (SELECT unnest(ARRAY[5,10,15,20,25,30,50,75,99,100]) AS id) k ON b.id = k.id;
 join_unnest_hit_count 
-----------------------
                    10
(1 row)

-- Probe with keys not in table (should return 0 matches)
SELECT count(*) AS join_unnest_miss_count
FROM clustered_pg_join_unnest_base b
JOIN (SELECT unnest(ARRAY[101,200,300]) AS id) k ON b.id = k.id;
 join_unnest_miss_count 
------------------------
                      0
(1 row)

-- Mixed hit/miss
SELECT count(*) AS join_unnest_mixed_count
FROM clustered_pg_join_unnest_base b
JOIN (SELECT unnest(ARRAY[1,50,100,101,200]) AS id) k ON b.id = k.id;
 join_unnest_mixed_count 
-------------------------
                       3
(1 row)

RESET enable_seqscan;
RESET enable_bitmapscan;
RESET enable_hashjoin;
RESET enable_mergejoin;
DROP TABLE clustered_pg_join_unnest_base;
-- Test delete + vacuum + re-query consistency
CREATE TABLE clustered_pg_vacuum_consistency(id bigint) USING clustered_heap;
CREATE INDEX clustered_pg_vacuum_consistency_idx
	ON clustered_pg_vacuum_consistency USING clustered_pk_index (id);
INSERT INTO clustered_pg_vacuum_consistency(id) SELECT generate_series(1,50);
-- Delete a range and verify count
DELETE FROM clustered_pg_vacuum_consistency WHERE id BETWEEN 10 AND 30;
SELECT count(*) AS vacuum_pre_vacuum_count FROM clustered_pg_vacuum_consistency;
 vacuum_pre_vacuum_count 
-------------------------
                      29
(1 row)

-- Vacuum and re-verify
VACUUM clustered_pg_vacuum_consistency;
SELECT count(*) AS vacuum_post_vacuum_count FROM clustered_pg_vacuum_consistency;
 vacuum_post_vacuum_count 
--------------------------
                       29
(1 row)

-- Verify remaining rows are correct
SELECT array_agg(id ORDER BY id) AS vacuum_remaining_ids
FROM clustered_pg_vacuum_consistency
WHERE id <= 15;
 vacuum_remaining_ids 
----------------------
 {1,2,3,4,5,6,7,8,9}
(1 row)

-- Re-verify data integrity after vacuum
SELECT count(*) AS vacuum_post_gc_count FROM clustered_pg_vacuum_consistency;
 vacuum_post_gc_count 
----------------------
                   29
(1 row)

DROP TABLE clustered_pg_vacuum_consistency;
-- Test segment split boundary: insert exactly at capacity edge
CREATE TABLE clustered_pg_split_edge(id bigint) USING clustered_heap;
CREATE INDEX clustered_pg_split_edge_idx
	ON clustered_pg_split_edge USING clustered_pk_index (id);
-- Insert exactly split_threshold rows
INSERT INTO clustered_pg_split_edge(id) SELECT generate_series(1,16);
SELECT count(*) AS split_edge_at_capacity FROM clustered_pg_split_edge;
 split_edge_at_capacity 
------------------------
                     16
(1 row)

-- Insert more rows
INSERT INTO clustered_pg_split_edge(id) SELECT generate_series(17,20);
SELECT count(*) AS split_edge_total_rows FROM clustered_pg_split_edge;
 split_edge_total_rows 
-----------------------
                    20
(1 row)

DROP TABLE clustered_pg_split_edge;
-- Test empty table operations
CREATE TABLE clustered_pg_empty(id bigint) USING clustered_heap;
CREATE INDEX clustered_pg_empty_idx
	ON clustered_pg_empty USING clustered_pk_index (id);
SET enable_seqscan = off;
SET enable_bitmapscan = off;
SELECT count(*) AS empty_count FROM clustered_pg_empty;
 empty_count 
-------------
           0
(1 row)

SELECT count(*) AS empty_filter_count FROM clustered_pg_empty WHERE id = 1;
 empty_filter_count 
--------------------
                  0
(1 row)

RESET enable_seqscan;
RESET enable_bitmapscan;
DROP TABLE clustered_pg_empty;
-- ================================================================
-- Directed placement: verify rows with same key land on same block
-- ================================================================
CREATE TABLE clustered_pg_directed(id int) USING clustered_heap;
CREATE INDEX clustered_pg_directed_idx
    ON clustered_pg_directed USING clustered_pk_index (id);
-- Insert 200 rows: 20 distinct keys, 10 rows each.
-- With directed placement, all 10 rows for the same key should land
-- on the same block (or very few blocks).
INSERT INTO clustered_pg_directed(id)
SELECT key_id
FROM generate_series(1, 20) AS key_id,
     generate_series(1, 10) AS rep;
-- For each key, count distinct blocks.  Perfect clustering = 1 block per key.
-- Allow up to 2 (page could fill up for large tuples).
SELECT
    CASE WHEN every(blk_count <= 2)
         THEN 'directed_placement_ok'
         ELSE 'directed_placement_FAIL'
    END AS directed_placement_result
FROM (
    SELECT id, count(DISTINCT (ctid::text::point)[0]::int) AS blk_count
    FROM clustered_pg_directed
    GROUP BY id
) sub;
 directed_placement_result 
---------------------------
 directed_placement_ok
(1 row)

-- Verify monotonic block ordering: keys inserted in order should have
-- non-decreasing minimum block numbers.
SELECT
    CASE WHEN bool_and(min_blk >= lag_blk OR lag_blk IS NULL)
         THEN 'block_order_ok'
         ELSE 'block_order_FAIL'
    END AS block_order_result
FROM (
    SELECT id,
           min((ctid::text::point)[0]::int) AS min_blk,
           lag(min((ctid::text::point)[0]::int)) OVER (ORDER BY id) AS lag_blk
    FROM clustered_pg_directed
    GROUP BY id
) sub;
 block_order_result 
--------------------
 block_order_ok
(1 row)

DROP TABLE clustered_pg_directed;
-- ================================================================
-- COPY path directed placement (multi_insert override)
-- ================================================================
CREATE TABLE clustered_pg_copy_dp(id int, payload text) USING clustered_heap;
CREATE INDEX clustered_pg_copy_dp_idx
    ON clustered_pg_copy_dp USING clustered_pk_index (id);
-- Use INSERT ... SELECT which goes through multi_insert for large batches
-- 30 keys x 30 rows each = 900 rows, ~500 byte payload -> multi-block
INSERT INTO clustered_pg_copy_dp(id, payload)
SELECT ((g % 30) + 1), repeat('x', 500)
FROM generate_series(1, 900) g;
-- With multi_insert directed placement, same-key rows should cluster
SELECT
    CASE WHEN avg(blk_count) <= 4.0
         THEN 'copy_directed_ok'
         ELSE 'copy_directed_FAIL'
    END AS copy_directed_result
FROM (
    SELECT id, count(DISTINCT (ctid::text::point)[0]::int) AS blk_count
    FROM clustered_pg_copy_dp
    GROUP BY id
) sub;
 copy_directed_result 
----------------------
 copy_directed_ok
(1 row)

DROP TABLE clustered_pg_copy_dp;
-- ================================================================
-- UPDATE + DELETE on directed-placement table
-- ================================================================
CREATE TABLE clustered_pg_upd(id int, val int, payload text) USING clustered_heap;
CREATE INDEX clustered_pg_upd_idx
    ON clustered_pg_upd USING clustered_pk_index (id);
-- Insert 5 keys, 10 rows each
INSERT INTO clustered_pg_upd(id, val, payload)
SELECT (g % 5) + 1, g, repeat('y', 200)
FROM generate_series(1, 50) g;
-- Verify initial count
SELECT count(*) AS before_count FROM clustered_pg_upd;
 before_count 
--------------
           50
(1 row)

-- UPDATE: change payload (same key = HOT candidate)
UPDATE clustered_pg_upd SET val = val + 1000 WHERE id = 3;
-- UPDATE: change the clustering key itself
UPDATE clustered_pg_upd SET id = 99 WHERE id = 5;
-- Verify counts by key after updates
SELECT id, count(*) AS cnt FROM clustered_pg_upd
WHERE id IN (3, 5, 99) GROUP BY id ORDER BY id;
 id | cnt 
----+-----
  3 |  10
 99 |  10
(2 rows)

-- DELETE a whole key group
DELETE FROM clustered_pg_upd WHERE id = 2;
-- Verify final count
SELECT count(*) AS after_count FROM clustered_pg_upd;
 after_count 
-------------
          40
(1 row)

-- Re-INSERT into the table (zone map should still work for new inserts)
INSERT INTO clustered_pg_upd(id, val, payload)
SELECT 2, g, repeat('z', 200)
FROM generate_series(1, 5) g;
SELECT count(*) AS final_count FROM clustered_pg_upd;
 final_count 
-------------
          45
(1 row)

DROP TABLE clustered_pg_upd;
-- ================================================================
-- NULL clustering key handling
-- ================================================================
CREATE TABLE clustered_pg_null(id int, payload text) USING clustered_heap;
CREATE INDEX clustered_pg_null_idx
    ON clustered_pg_null USING clustered_pk_index (id);
-- Insert rows with NULL key: directed placement skips NULL keys (safe fallback),
-- but the index AM rejects NULLs with an error.  These INSERTs are expected to fail.
INSERT INTO clustered_pg_null(id, payload) VALUES (NULL, 'null_row_1');
ERROR:  clustered_pk_index currently supports only int2/int4/int8 index key types
DETAIL:  Index key is NULL, missing, or has unsupported type.
INSERT INTO clustered_pg_null(id, payload) VALUES (NULL, 'null_row_2');
ERROR:  clustered_pk_index currently supports only int2/int4/int8 index key types
DETAIL:  Index key is NULL, missing, or has unsupported type.
INSERT INTO clustered_pg_null(id, payload) VALUES (1, 'normal_row');
SELECT count(*) AS null_test_count FROM clustered_pg_null;
 null_test_count 
-----------------
               1
(1 row)

-- Verify we can read back all rows including NULLs
SELECT id IS NULL AS is_null, count(*) AS cnt
FROM clustered_pg_null GROUP BY (id IS NULL) ORDER BY is_null;
 is_null | cnt 
---------+-----
 f       |   1
(1 row)

DROP TABLE clustered_pg_null;
-- ================================================================
-- Directed placement with many distinct keys (fast path exercise)
-- ================================================================
CREATE TABLE clustered_pg_many(id int, payload text) USING clustered_heap;
CREATE INDEX clustered_pg_many_idx
    ON clustered_pg_many USING clustered_pk_index (id);
-- 200 distinct keys x 5 rows each = 1000 rows, triggers fast path (>64 keys)
INSERT INTO clustered_pg_many(id, payload)
SELECT ((g % 200) + 1), repeat('m', 100)
FROM generate_series(1, 1000) g;
SELECT
    CASE WHEN count(*) = 1000
         THEN 'many_keys_count_ok'
         ELSE 'many_keys_count_FAIL'
    END AS many_keys_result
FROM clustered_pg_many;
  many_keys_result  
--------------------
 many_keys_count_ok
(1 row)

-- Even with fast path, zone map should provide some clustering benefit
-- (not as tight as group path, but better than random heap)
SELECT
    CASE WHEN avg(blk_count) <= 10.0
         THEN 'many_keys_scatter_ok'
         ELSE 'many_keys_scatter_FAIL'
    END AS many_keys_scatter_result
FROM (
    SELECT id, count(DISTINCT (ctid::text::point)[0]::int) AS blk_count
    FROM clustered_pg_many
    GROUP BY id
) sub;
 many_keys_scatter_result 
--------------------------
 many_keys_scatter_ok
(1 row)

DROP TABLE clustered_pg_many;
-- ================================================================
-- VACUUM on directed-placement table (delete + vacuum + re-insert)
-- ================================================================
CREATE TABLE clustered_pg_vac_dp(id int, payload text) USING clustered_heap;
CREATE INDEX clustered_pg_vac_dp_idx
    ON clustered_pg_vac_dp USING clustered_pk_index (id);
-- 10 keys x 50 rows = 500 rows
INSERT INTO clustered_pg_vac_dp(id, payload)
SELECT ((g % 10) + 1), repeat('v', 200)
FROM generate_series(1, 500) g;
SELECT count(*) AS vac_before FROM clustered_pg_vac_dp;
 vac_before 
------------
        500
(1 row)

-- Delete 60% of rows
DELETE FROM clustered_pg_vac_dp WHERE id <= 6;
VACUUM clustered_pg_vac_dp;
-- Verify remaining rows survived vacuum
SELECT count(*) AS vac_after FROM clustered_pg_vac_dp;
 vac_after 
-----------
       200
(1 row)

-- Re-insert: directed placement should still cluster new rows
INSERT INTO clustered_pg_vac_dp(id, payload)
SELECT ((g % 6) + 1), repeat('w', 200)
FROM generate_series(1, 300) g;
-- Verify data integrity and count
SELECT
    CASE WHEN count(*) = 500
         THEN 'vac_reinsert_ok'
         ELSE 'vac_reinsert_FAIL'
    END AS vac_reinsert_result
FROM clustered_pg_vac_dp;
 vac_reinsert_result 
---------------------
 vac_reinsert_ok
(1 row)

DROP TABLE clustered_pg_vac_dp;
-- ================================================================
-- TRUNCATE invalidates zone map (re-insert should not crash)
-- ================================================================
CREATE TABLE clustered_pg_trunc(id int, payload text) USING clustered_heap;
CREATE INDEX clustered_pg_trunc_idx
    ON clustered_pg_trunc USING clustered_pk_index (id);
INSERT INTO clustered_pg_trunc(id, payload)
SELECT g, repeat('t', 100)
FROM generate_series(1, 100) g;
SELECT count(*) AS before_trunc FROM clustered_pg_trunc;
 before_trunc 
--------------
          100
(1 row)

TRUNCATE clustered_pg_trunc;
SELECT count(*) AS after_trunc FROM clustered_pg_trunc;
 after_trunc 
-------------
           0
(1 row)

-- Re-insert after truncate: zone map was invalidated, should rebuild cleanly
INSERT INTO clustered_pg_trunc(id, payload)
SELECT g, repeat('u', 100)
FROM generate_series(1, 50) g;
SELECT
    CASE WHEN count(*) = 50
         THEN 'trunc_reinsert_ok'
         ELSE 'trunc_reinsert_FAIL'
    END AS trunc_reinsert_result
FROM clustered_pg_trunc;
 trunc_reinsert_result 
-----------------------
 trunc_reinsert_ok
(1 row)

DROP TABLE clustered_pg_trunc;
-- ================================================================
-- JOIN with btree on directed-placement table (production pattern)
-- ================================================================
-- This is the key scenario: standard btree serves JOINs efficiently
-- because directed placement physically clusters rows by key.
CREATE TABLE clustered_pg_join(id int, payload text) USING clustered_heap;
CREATE INDEX clustered_pg_join_pkidx
    ON clustered_pg_join USING clustered_pk_index (id);
CREATE INDEX clustered_pg_join_btree
    ON clustered_pg_join USING btree (id);
-- 1000 keys x 100 rows = 100K rows, interleaved insert
INSERT INTO clustered_pg_join(id, payload)
SELECT ((g % 1000) + 1), repeat('j', 100)
FROM generate_series(1, 100000) g;
ANALYZE clustered_pg_join;
-- Verify the btree index is used (not clustered_pk_index)
-- Nested loop + index scan should touch very few blocks per key
SELECT
    CASE WHEN count(*) = 20000
         THEN 'join_btree_count_ok'
         ELSE 'join_btree_count_FAIL'
    END AS join_btree_result
FROM clustered_pg_join d
JOIN (SELECT unnest(ARRAY(SELECT generate_series(1, 200))) AS id) keys
ON d.id = keys.id;
  join_btree_result  
---------------------
 join_btree_count_ok
(1 row)

-- Verify clustering held through the full insert:
-- each key's rows should be on <=5 blocks
SELECT
    CASE WHEN avg(blk_count) <= 5.0
         THEN 'join_btree_scatter_ok'
         ELSE 'join_btree_scatter_FAIL'
    END AS join_btree_scatter_result
FROM (
    SELECT id, count(DISTINCT (ctid::text::point)[0]::int) AS blk_count
    FROM clustered_pg_join
    GROUP BY id
) sub;
 join_btree_scatter_result 
---------------------------
 join_btree_scatter_ok
(1 row)

DROP TABLE clustered_pg_join;
-- ================================================================
-- sorted_heap Table AM: Phase 1 tests
-- ================================================================
-- Test SH-1: Create table with sorted_heap AM
CREATE TABLE sh_basic(id bigint, val text) USING sorted_heap;
-- Test SH-2: Single INSERT + SELECT
INSERT INTO sh_basic(id, val) VALUES (1, 'hello');
SELECT count(*) AS sh_single_count FROM sh_basic;
 sh_single_count 
-----------------
               1
(1 row)

-- Test SH-3: Bulk INSERT
INSERT INTO sh_basic(id, val)
SELECT g, 'row_' || g FROM generate_series(2, 100) g;
SELECT count(*) AS sh_multi_count FROM sh_basic;
 sh_multi_count 
----------------
            100
(1 row)

-- Test SH-4: Data roundtrip (correct values returned)
SELECT id, val FROM sh_basic WHERE id = 50;
 id |  val   
----+--------
 50 | row_50
(1 row)

-- Test SH-5: DELETE
DELETE FROM sh_basic WHERE id BETWEEN 20 AND 30;
SELECT count(*) AS sh_after_delete FROM sh_basic;
 sh_after_delete 
-----------------
              89
(1 row)

-- Test SH-6: UPDATE
UPDATE sh_basic SET val = 'updated' WHERE id = 1;
SELECT val AS sh_updated_val FROM sh_basic WHERE id = 1;
 sh_updated_val 
----------------
 updated
(1 row)

-- Test SH-7: VACUUM
VACUUM sh_basic;
SELECT count(*) AS sh_after_vacuum FROM sh_basic;
 sh_after_vacuum 
-----------------
              89
(1 row)

-- Test SH-8: Index creation and index scan
CREATE INDEX sh_basic_idx ON sh_basic USING btree (id);
SET enable_seqscan = off;
SELECT count(*) AS sh_idx_count FROM sh_basic WHERE id = 50;
 sh_idx_count 
--------------
            1
(1 row)

RESET enable_seqscan;
-- Test SH-9: TRUNCATE + re-insert
TRUNCATE sh_basic;
SELECT count(*) AS sh_after_trunc FROM sh_basic;
 sh_after_trunc 
----------------
              0
(1 row)

INSERT INTO sh_basic(id, val) VALUES (1, 'after_truncate');
SELECT count(*) AS sh_reinsert FROM sh_basic;
 sh_reinsert 
-------------
           1
(1 row)

DROP TABLE sh_basic;
-- Test SH-10: Empty table
CREATE TABLE sh_empty(id bigint) USING sorted_heap;
SELECT count(*) AS sh_empty FROM sh_empty;
 sh_empty 
----------
        0
(1 row)

DROP TABLE sh_empty;
-- Test SH-11: Bulk multi-insert path (large batch)
CREATE TABLE sh_bulk(id int, payload text) USING sorted_heap;
INSERT INTO sh_bulk(id, payload)
SELECT g, repeat('x', 200) FROM generate_series(1, 1000) g;
SELECT count(*) AS sh_bulk_count FROM sh_bulk;
 sh_bulk_count 
---------------
          1000
(1 row)

SELECT count(*) AS sh_bulk_range
FROM sh_bulk WHERE id BETWEEN 500 AND 510;
 sh_bulk_range 
---------------
            11
(1 row)

DROP TABLE sh_bulk;
-- Test SH-12: ANALYZE
CREATE TABLE sh_analyze(id bigint, val text) USING sorted_heap;
INSERT INTO sh_analyze(id, val)
SELECT g, repeat('a', 100) FROM generate_series(1, 500) g;
ANALYZE sh_analyze;
SELECT count(*) AS sh_post_analyze FROM sh_analyze;
 sh_post_analyze 
-----------------
             500
(1 row)

DROP TABLE sh_analyze;
-- Test SH-13: NULL values
CREATE TABLE sh_null(id bigint, val text) USING sorted_heap;
INSERT INTO sh_null(id, val) VALUES (NULL, 'null_id');
INSERT INTO sh_null(id, val) VALUES (1, NULL);
INSERT INTO sh_null(id, val) VALUES (NULL, NULL);
SELECT count(*) AS sh_null_count FROM sh_null;
 sh_null_count 
---------------
             3
(1 row)

DROP TABLE sh_null;
-- Test SH-14: Co-existence with clustered_heap
CREATE TABLE ch_coexist(id int, payload text) USING clustered_heap;
CREATE INDEX ch_coexist_pkidx ON ch_coexist USING clustered_pk_index (id);
CREATE INDEX ch_coexist_idx ON ch_coexist USING btree (id);
CREATE TABLE sh_coexist(id int, payload text) USING sorted_heap;
INSERT INTO ch_coexist(id, payload) SELECT g, 'ch_' || g FROM generate_series(1, 10) g;
INSERT INTO sh_coexist(id, payload) SELECT g, 'sh_' || g FROM generate_series(1, 10) g;
SELECT
    (SELECT count(*) FROM ch_coexist) AS ch_count,
    (SELECT count(*) FROM sh_coexist) AS sh_count;
 ch_count | sh_count 
----------+----------
       10 |       10
(1 row)

DROP TABLE ch_coexist;
DROP TABLE sh_coexist;
-- Test SH-15: COPY path (exercises multi_insert)
CREATE TABLE sh_copy(id int, val text) USING sorted_heap;
COPY sh_copy FROM stdin;
SELECT count(*) AS sh_copy_count FROM sh_copy;
 sh_copy_count 
---------------
             5
(1 row)

DROP TABLE sh_copy;
-- ================================================================
-- sorted_heap Table AM: Phase 2 tests (PK-sorted COPY)
-- ================================================================
-- Test SH2-1: COPY with int PK — verify physical sort order
CREATE TABLE sh2_pk_int(id int PRIMARY KEY, val text) USING sorted_heap;
-- Generate data in reverse order, COPY into sorted_heap
CREATE TEMP TABLE sh2_src1 AS
    SELECT id, 'v' || id AS val FROM generate_series(1, 500) id ORDER BY id DESC;
COPY sh2_src1 TO '/tmp/sh2_pk_int.csv' CSV;
COPY sh2_pk_int FROM '/tmp/sh2_pk_int.csv' CSV;
-- Verify zero inversions in physical order vs PK order
SELECT
    CASE WHEN count(*) = 0
         THEN 'pk_int_sorted_ok'
         ELSE 'pk_int_sorted_FAIL'
    END AS sh2_pk_int_result
FROM (
    SELECT id < lag(id) OVER (ORDER BY ctid) AS inv
    FROM sh2_pk_int
) sub
WHERE inv;
 sh2_pk_int_result 
-------------------
 pk_int_sorted_ok
(1 row)

SELECT count(*) AS sh2_pk_int_count FROM sh2_pk_int;
 sh2_pk_int_count 
------------------
              500
(1 row)

DROP TABLE sh2_pk_int;
DROP TABLE sh2_src1;
-- Test SH2-2: COPY with composite PK (text, int)
CREATE TABLE sh2_composite(cat text, id int, val text, PRIMARY KEY(cat, id)) USING sorted_heap;
CREATE TEMP TABLE sh2_src2 AS
    SELECT chr(65 + (g % 5)) AS cat, g AS id, 'v' || g AS val
    FROM generate_series(1, 200) g
    ORDER BY random();
COPY sh2_src2 TO '/tmp/sh2_composite.csv' CSV;
COPY sh2_composite FROM '/tmp/sh2_composite.csv' CSV;
-- Verify sort: cat ASC, then id ASC within cat
SELECT
    CASE WHEN count(*) = 0
         THEN 'composite_sorted_ok'
         ELSE 'composite_sorted_FAIL'
    END AS sh2_composite_result
FROM (
    SELECT (cat < lag(cat) OVER (ORDER BY ctid))
        OR (cat = lag(cat) OVER (ORDER BY ctid)
            AND id < lag(id) OVER (ORDER BY ctid)) AS inv
    FROM sh2_composite
) sub
WHERE inv;
 sh2_composite_result 
----------------------
 composite_sorted_ok
(1 row)

SELECT count(*) AS sh2_composite_count FROM sh2_composite;
 sh2_composite_count 
---------------------
                 200
(1 row)

DROP TABLE sh2_composite;
DROP TABLE sh2_src2;
-- Test SH2-3: COPY without PK (no crash, works as heap)
CREATE TABLE sh2_nopk(id int, val text) USING sorted_heap;
CREATE TEMP TABLE sh2_src3 AS
    SELECT g AS id, 'v' || g AS val FROM generate_series(1, 100) g ORDER BY random();
COPY sh2_src3 TO '/tmp/sh2_nopk.csv' CSV;
COPY sh2_nopk FROM '/tmp/sh2_nopk.csv' CSV;
SELECT count(*) AS sh2_nopk_count FROM sh2_nopk;
 sh2_nopk_count 
----------------
            100
(1 row)

DROP TABLE sh2_nopk;
DROP TABLE sh2_src3;
-- Test SH2-4: PK created after table — relcache callback triggers re-detection
CREATE TABLE sh2_latepk(id int, val text) USING sorted_heap;
-- COPY without PK (unsorted)
CREATE TEMP TABLE sh2_src4 AS
    SELECT g AS id, 'v' || g AS val FROM generate_series(1, 50) g ORDER BY random();
COPY sh2_src4 TO '/tmp/sh2_latepk.csv' CSV;
COPY sh2_latepk FROM '/tmp/sh2_latepk.csv' CSV;
-- Now add PK
ALTER TABLE sh2_latepk ADD PRIMARY KEY (id);
-- COPY more data — this batch should be sorted
TRUNCATE sh2_src4;
INSERT INTO sh2_src4 SELECT g, 'w' || g FROM generate_series(51, 100) g ORDER BY random();
COPY sh2_src4 TO '/tmp/sh2_latepk2.csv' CSV;
COPY sh2_latepk FROM '/tmp/sh2_latepk2.csv' CSV;
-- Verify second batch is sorted (filter by id > 50 to check only new rows)
SELECT
    CASE WHEN count(*) = 0
         THEN 'latepk_sorted_ok'
         ELSE 'latepk_sorted_FAIL'
    END AS sh2_latepk_result
FROM (
    SELECT id < lag(id) OVER (ORDER BY ctid) AS inv
    FROM sh2_latepk
    WHERE id > 50
) sub
WHERE inv;
 sh2_latepk_result 
-------------------
 latepk_sorted_ok
(1 row)

SELECT count(*) AS sh2_latepk_count FROM sh2_latepk;
 sh2_latepk_count 
------------------
              100
(1 row)

DROP TABLE sh2_latepk;
DROP TABLE sh2_src4;
-- Test SH2-5: COPY with text PK (collation-aware sort)
CREATE TABLE sh2_textpk(name text PRIMARY KEY, val int) USING sorted_heap;
CREATE TEMP TABLE sh2_src5 AS
    SELECT 'item_' || lpad(g::text, 4, '0') AS name, g AS val
    FROM generate_series(1, 200) g
    ORDER BY random();
COPY sh2_src5 TO '/tmp/sh2_textpk.csv' CSV;
COPY sh2_textpk FROM '/tmp/sh2_textpk.csv' CSV;
SELECT
    CASE WHEN count(*) = 0
         THEN 'textpk_sorted_ok'
         ELSE 'textpk_sorted_FAIL'
    END AS sh2_textpk_result
FROM (
    SELECT name < lag(name) OVER (ORDER BY ctid) AS inv
    FROM sh2_textpk
) sub
WHERE inv;
 sh2_textpk_result 
-------------------
 textpk_sorted_ok
(1 row)

SELECT count(*) AS sh2_textpk_count FROM sh2_textpk;
 sh2_textpk_count 
------------------
              200
(1 row)

DROP TABLE sh2_textpk;
DROP TABLE sh2_src5;
-- Test SH2-6: COPY with NULLs in non-PK columns (no crash)
CREATE TABLE sh2_nulls(id int PRIMARY KEY, val text) USING sorted_heap;
CREATE TEMP TABLE sh2_src6(id int, val text);
INSERT INTO sh2_src6 VALUES (5, NULL), (3, 'three'), (1, NULL), (4, 'four'), (2, NULL);
COPY sh2_src6 TO '/tmp/sh2_nulls.csv' CSV;
COPY sh2_nulls FROM '/tmp/sh2_nulls.csv' CSV;
-- Still sorted by PK
SELECT
    CASE WHEN count(*) = 0
         THEN 'nulls_sorted_ok'
         ELSE 'nulls_sorted_FAIL'
    END AS sh2_nulls_result
FROM (
    SELECT id < lag(id) OVER (ORDER BY ctid) AS inv
    FROM sh2_nulls
) sub
WHERE inv;
 sh2_nulls_result 
------------------
 nulls_sorted_ok
(1 row)

SELECT count(*) AS sh2_nulls_count FROM sh2_nulls;
 sh2_nulls_count 
-----------------
               5
(1 row)

DROP TABLE sh2_nulls;
DROP TABLE sh2_src6;
-- Test SH2-7: INSERT...SELECT (tuple_insert path) — works, no crash
CREATE TABLE sh2_insert_sel(id int PRIMARY KEY, val text) USING sorted_heap;
INSERT INTO sh2_insert_sel SELECT g, 'v' || g FROM generate_series(1, 100) g;
SELECT count(*) AS sh2_insert_sel_count FROM sh2_insert_sel;
 sh2_insert_sel_count 
----------------------
                  100
(1 row)

DROP TABLE sh2_insert_sel;
-- Test SH2-8: Single inserts still work after Phase 2 changes
CREATE TABLE sh2_singles(id int PRIMARY KEY, val text) USING sorted_heap;
INSERT INTO sh2_singles VALUES (5, 'e');
INSERT INTO sh2_singles VALUES (3, 'c');
INSERT INTO sh2_singles VALUES (1, 'a');
INSERT INTO sh2_singles VALUES (4, 'd');
INSERT INTO sh2_singles VALUES (2, 'b');
SELECT count(*) AS sh2_singles_count FROM sh2_singles;
 sh2_singles_count 
-------------------
                 5
(1 row)

-- Verify data roundtrip
SELECT id, val FROM sh2_singles ORDER BY id;
 id | val 
----+-----
  1 | a
  2 | b
  3 | c
  4 | d
  5 | e
(5 rows)

DROP TABLE sh2_singles;
-- Test SH2-9: COPY + VACUUM + more COPY (PK cache survives)
CREATE TABLE sh2_vac(id int PRIMARY KEY, val text) USING sorted_heap;
CREATE TEMP TABLE sh2_src9 AS
    SELECT g AS id, 'v' || g AS val FROM generate_series(1, 200) g ORDER BY random();
COPY sh2_src9 TO '/tmp/sh2_vac.csv' CSV;
COPY sh2_vac FROM '/tmp/sh2_vac.csv' CSV;
DELETE FROM sh2_vac WHERE id <= 100;
VACUUM sh2_vac;
-- Second COPY after vacuum
TRUNCATE sh2_src9;
INSERT INTO sh2_src9 SELECT g, 'w' || g FROM generate_series(201, 400) g ORDER BY random();
COPY sh2_src9 TO '/tmp/sh2_vac2.csv' CSV;
COPY sh2_vac FROM '/tmp/sh2_vac2.csv' CSV;
SELECT
    CASE WHEN count(*) = 0
         THEN 'vac_sorted_ok'
         ELSE 'vac_sorted_FAIL'
    END AS sh2_vac_result
FROM (
    SELECT id < lag(id) OVER (ORDER BY ctid) AS inv
    FROM sh2_vac
    WHERE id > 200
) sub
WHERE inv;
 sh2_vac_result 
----------------
 vac_sorted_ok
(1 row)

SELECT count(*) AS sh2_vac_count FROM sh2_vac;
 sh2_vac_count 
---------------
           300
(1 row)

DROP TABLE sh2_vac;
DROP TABLE sh2_src9;
DROP EXTENSION clustered_pg;
